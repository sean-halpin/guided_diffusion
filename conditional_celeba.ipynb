{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/guided_diffusion/blob/main/conditional_celeba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "KC6FJYuwvRzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continue_training = True"
      ],
      "metadata": {
        "id": "HkwppPi9XqeI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, os.path\n",
        "\n",
        "first_run = True\n",
        "if os.path.exists('/content/data/celeba/img_align_celeba') and len(os.listdir('/content/data/celeba/img_align_celeba')) == 202599:\n",
        "  first_run = False"
      ],
      "metadata": {
        "id": "XUIaumn_zhLt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BNY6aXhoAnkb"
      },
      "outputs": [],
      "source": [
        "if first_run:\n",
        "  !rm -rf /content/images/*\n",
        "  !mkdir -p /content/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bIvCv_xS4nR3"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip uninstall matplotlib -y\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "KbaCeOgGtHsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9K3x1nQI2BS"
      },
      "source": [
        "### UNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "ldERGcvx4SjK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers[training]==0.1.3"
      ],
      "metadata": {
        "id": "75jejDLcKeC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel"
      ],
      "metadata": {
        "id": "_8ZSJZ_uLaZ4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "KhnJvTm4n_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CelebA"
      ],
      "metadata": {
        "id": "6oWvcCsVvAzz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2eEu3ZVZGJVa"
      },
      "outputs": [],
      "source": [
        "x_size = 32\n",
        "y_size = x_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rBJeO_gPGDsK"
      },
      "outputs": [],
      "source": [
        "tf = transforms.Compose(\n",
        "  [\n",
        "    transforms.Resize((x_size, y_size)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if first_run:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  !mkdir -p ./data\n",
        "  !rm -rf ./data/*\n",
        "  !cp -R ./drive/MyDrive/celeba/* ./data/celeba\n",
        "  !unzip \"./data/celeba/img_align_celeba.zip\" -d \"./data/celeba\""
      ],
      "metadata": {
        "id": "XHhMYmkYrqRi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1qLCertIF9wj"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "dataset = None\n",
        "result = None\n",
        "while result is None:\n",
        "    try:\n",
        "      # connect\n",
        "      dataset = CelebA(\n",
        "        \"./data/\",\n",
        "        download=False,\n",
        "        transform=tf,\n",
        "      )\n",
        "      result = True\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # break\n",
        "      time.sleep(60)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K3j-SLDjnDI0"
      },
      "outputs": [],
      "source": [
        "subset = torch.utils.data.Subset(dataset, range(0,320))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_ = 16\n",
        "eval_batch_size_ = 1"
      ],
      "metadata": {
        "id": "lz-S15b6AFU_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader = DataLoader(subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size_, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "qLZ0wYHb3hGS"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qA4jjn7I43w"
      },
      "source": [
        "### Denoising Diffusion Probabilistic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Accelerate"
      ],
      "metadata": {
        "id": "uhwbA4szWTjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "\n",
        "\n",
        "class DDPMConditionalPipeline(DiffusionPipeline):\n",
        "    def __init__(self, unet: UNet2DConditionModel, scheduler):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(unet=unet, scheduler=scheduler)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, batch_size=1, generator=None, torch_device=None, output_type=\"pil\", hidden_states=None):\n",
        "        if torch_device is None:\n",
        "            torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        self.unet.to(torch_device)\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        image = torch.randn(\n",
        "            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n",
        "            generator=generator,\n",
        "        )\n",
        "        image = image.to(torch_device)\n",
        "\n",
        "        # set step values\n",
        "        self.scheduler.set_timesteps(1000)\n",
        "\n",
        "        for t in tqdm(self.scheduler.timesteps):\n",
        "            # 1. predict noise model_output\n",
        "            model_output = self.unet(image, t, hidden_states)[\"sample\"]\n",
        "\n",
        "            # 2. compute previous image: x_t -> t_t-1\n",
        "            image = self.scheduler.step(model_output, t, image)[\"prev_sample\"]\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}"
      ],
      "metadata": {
        "id": "6FAjtnErlOul"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26fo9mj0f9rP",
        "outputId": "c2fd76b2-d511-4bb5-f719-e6f1bab48ca3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "\n",
        "emb = nn.Embedding(2, 40, padding_idx=0)\n",
        "lin = nn.Linear(40, 1280)\n",
        "\n",
        "class DDPMConditionalPipeline(DiffusionPipeline):\n",
        "    def __init__(self, unet: UNet2DConditionModel, scheduler):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(unet=unet, scheduler=scheduler)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, guidance_scale=1.5, batch_size=1, generator=None, torch_device=None, output_type=\"pil\", hidden_states=None):\n",
        "        if torch_device is None:\n",
        "            torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        self.unet.to(torch_device)\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        latents = torch.randn(\n",
        "            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n",
        "            generator=generator,\n",
        "        )\n",
        "        image = latents.to(torch_device)\n",
        "\n",
        "        # set step values\n",
        "        self.scheduler.set_timesteps(1000)\n",
        "\n",
        "        for t in tqdm(self.scheduler.timesteps):\n",
        "            # 1. predict noise noise_pred\n",
        "            # latents_input = torch.cat([latents] * 2)\n",
        "            no_condition_attrs = torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).long()\n",
        "            y = lin(emb(no_condition_attrs)).to(torch_device)\n",
        "            # context = torch.cat([[y], [hidden_states]])\n",
        "            noise_prediction_text = self.unet(image, t, hidden_states)[\"sample\"]\n",
        "            noise_pred_uncond = self.unet(image, t, y)[\"sample\"]\n",
        "\n",
        "            # noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
        "\n",
        "\n",
        "            # 2. compute previous image: x_t -> t_t-1\n",
        "            image = self.scheduler.step(noise_pred, t, image)[\"prev_sample\"]\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}"
      ],
      "metadata": {
        "id": "prqVe251HpjY"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "# from datasets import load_dataset\n",
        "# from diffusers import DDPMPipeline, DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    InterpolationMode,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "IdWgjyeuWRjJ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p logs"
      ],
      "metadata": {
        "id": "GbxPdIHFW4Lj"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"CelebA\")\n",
        "parser.add_argument(\"--dataset_name\", type=str, default=\"CelebA\")\n",
        "parser.add_argument(\"--dataset_config_name\", type=str, default=None)\n",
        "parser.add_argument(\"--train_data_dir\", type=str, default=None, help=\"A folder containing the training data.\")\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"diffusion_conditional\")\n",
        "parser.add_argument(\"--overwrite_output_dir\", action=\"store_true\")\n",
        "parser.add_argument(\"--cache_dir\", type=str, default=None)\n",
        "parser.add_argument(\"--resolution\", type=int, default=x_size)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=batch_size_)\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=eval_batch_size_)\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=100)\n",
        "parser.add_argument(\"--save_images_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--save_model_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--lr_scheduler\", type=str, default=\"cosine\")\n",
        "parser.add_argument(\"--lr_warmup_steps\", type=int, default=500)\n",
        "parser.add_argument(\"--adam_beta1\", type=float, default=0.95)\n",
        "parser.add_argument(\"--adam_beta2\", type=float, default=0.999)\n",
        "parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-6)\n",
        "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08)\n",
        "parser.add_argument(\"--use_ema\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--ema_inv_gamma\", type=float, default=1.0)\n",
        "parser.add_argument(\"--ema_power\", type=float, default=3 / 4)\n",
        "parser.add_argument(\"--ema_max_decay\", type=float, default=0.9999)\n",
        "parser.add_argument(\"--push_to_hub\", default=True)\n",
        "parser.add_argument(\"--use_auth_token\", action=\"store_true\")\n",
        "parser.add_argument(\"--hub_token\", type=str, default=None)\n",
        "parser.add_argument(\"--hub_model_id\", type=str, default=\"diffusion_conditional\")\n",
        "parser.add_argument(\"--hub_private_repo\", default=False)\n",
        "parser.add_argument(\"--logging_dir\", type=str, default=\"logs\")\n",
        "parser.add_argument(\n",
        "    \"--mixed_precision\",\n",
        "    type=str,\n",
        "    default=\"fp16\",\n",
        "    choices=[\"no\", \"fp16\", \"bf16\"],\n",
        "    help=(\n",
        "        \"Whether to use mixed precision. Choose\"\n",
        "        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
        "        \"and an Nvidia Ampere GPU.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "TMK26PrLXM7_"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WuemTUnjbJMU"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging_dir = os.path.join('.', '/logs')\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=args.mixed_precision,\n",
        "    log_with=\"tensorboard\",\n",
        "    logging_dir=logging_dir,\n",
        ")"
      ],
      "metadata": {
        "id": "YLuUoiMY5XB4"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet2DConditionModel(\n",
        "    sample_size=args.resolution,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 128, 256, 256),\n",
        "    # block_out_channels=(128, 128, 256, 256, 512, 512),\n",
        "    down_block_types=( \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"DownBlock2D\"\n",
        "    ), \n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\"\n",
        "      ),\n",
        ")\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")"
      ],
      "metadata": {
        "id": "iijTrsJqVLXS"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "if continue_training:\n",
        "  previous_model = \\\n",
        "    DDPMConditionalPipeline.from_pretrained('shalpin87/diffusion_conditional')\n",
        "  model = previous_model.unet\n",
        "  noise_scheduler = previous_model.scheduler"
      ],
      "metadata": {
        "id": "BHoPGLVtXnkE"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=(args.adam_beta1, args.adam_beta2),\n",
        "    weight_decay=args.adam_weight_decay,\n",
        "    eps=args.adam_epsilon,\n",
        ")\n",
        "\n",
        "train_dataloader = dataloader\n",
        "lr_scheduler = get_scheduler(\n",
        "    args.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * args.num_epochs) // args.gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "ema_model = EMAModel(model, inv_gamma=args.ema_inv_gamma, power=args.ema_power, max_value=args.ema_max_decay)"
      ],
      "metadata": {
        "id": "2RUtbgXkgUQ4"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.push_to_hub:\n",
        "    repo = init_git_repo(args, at_init=True)\n",
        "    repo.git_pull()"
      ],
      "metadata": {
        "id": "qC_zkxKK5eJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993c1fd8-86cf-46a4-d556-bd5d8358884a"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/diffusion_conditional is already a clone of https://huggingface.co/shalpin87/diffusion_conditional. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/diffusion_conditional is already a clone of https://huggingface.co/shalpin87/diffusion_conditional. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if accelerator.is_main_process:\n",
        "    run = os.path.split('.')[-1].split(\".\")[0]\n",
        "    accelerator.init_trackers(run)"
      ],
      "metadata": {
        "id": "accQ1syDH8g-"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, y in (train_dataloader):\n",
        "  print(batch.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "aOtrmIqg2RRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08d3a20-ba23-43fb-80b7-4153478361b5"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 32, 32])\n",
            "torch.Size([16, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0\n",
        "emb = nn.Embedding(2, 40, padding_idx=0)\n",
        "lin = nn.Linear(40, 1280)\n",
        "for epoch in range(args.num_epochs):\n",
        "    model.train()\n",
        "    progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "    # for step, batch in enumerate(train_dataloader):\n",
        "    for batch, batch_y in (train_dataloader):\n",
        "        # clean_images = batch[\"input\"]\n",
        "        clean_images = batch #[\"input\"]\n",
        "        # Sample noise that we'll add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "        bsz = clean_images.shape[0]\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_train_timesteps, (bsz,), device=clean_images.device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            # Predict the noise residual          \n",
        "            y = (lin(emb(batch_y.to(\"cpu\")))).to(\"cuda\")\n",
        "            noise_pred = model(noisy_images, timesteps, y)[\"sample\"]\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            if args.use_ema:\n",
        "                ema_model.step(model)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "        if args.use_ema:\n",
        "            logs[\"ema_decay\"] = ema_model.decay\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        accelerator.log(logs, step=global_step)\n",
        "        global_step += 1\n",
        "    progress_bar.close()\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # Generate sample images for visual inspection\n",
        "    # if accelerator.is_main_process:\n",
        "    if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        pipeline = DDPMConditionalPipeline(\n",
        "            unet=accelerator.unwrap_model(ema_model.averaged_model if args.use_ema else model),\n",
        "            scheduler=noise_scheduler,\n",
        "        )\n",
        "\n",
        "        generator = torch.manual_seed(0)\n",
        "        # run pipeline in inference (sample random noise and denoise)\n",
        "        attr = torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).long().repeat(eval_batch_size_,1)\n",
        "        y = (lin(emb(attr)))\n",
        "        images = pipeline(generator=generator, batch_size=args.eval_batch_size, output_type=\"numpy\", hidden_states=y.to(\"cuda\"))[\"sample\"]\n",
        "\n",
        "        # denormalize the images and save to tensorboard\n",
        "        images_processed = (images * 255).round().astype(\"uint8\")\n",
        "        imgs_t = images_processed.transpose(0, 3, 1, 2)\n",
        "        imgs_plt = images_processed.transpose(0, 1, 2, 3)\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        columns = 1\n",
        "        rows = 1\n",
        "        for i in range(1, columns*rows +1):\n",
        "            img = imgs_plt[i-1]\n",
        "            fig.add_subplot(rows, columns, i)\n",
        "            plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        # save the model\n",
        "        # if args.push_to_hub:\n",
        "        # push_to_hub(args, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=False)\n",
        "        # else:\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "xDV9MhaIq20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZLjKBSjJk1N"
      },
      "source": [
        "### Misc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = DDPMConditionalPipeline(\n",
        "    unet=accelerator.unwrap_model(ema_model.averaged_model if args.use_ema else model),\n",
        "    scheduler=noise_scheduler,\n",
        ")\n",
        "\n",
        "generator = torch.manual_seed(0)\n",
        "# run pipeline in inference (sample random noise and denoise)\n",
        "attr = torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]).long().repeat(eval_batch_size_,1)\n",
        "y = (lin(emb(attr)))\n",
        "images = pipeline(guidance_scale=250.0, generator=generator, batch_size=args.eval_batch_size, output_type=\"numpy\", hidden_states=y.to(\"cuda\"))[\"sample\"]\n",
        "\n",
        "# denormalize the images and save to tensorboard\n",
        "images_processed = (images * 255).round().astype(\"uint8\")\n",
        "imgs_t = images_processed.transpose(0, 3, 1, 2)\n",
        "imgs_plt = images_processed.transpose(0, 1, 2, 3)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "columns = 1\n",
        "rows = 1\n",
        "for i in range(1, columns*rows +1):\n",
        "    img = imgs_plt[i-1]\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cJfeb7Colrfi",
        "outputId": "e19cdd76-016f-48a1-ac10-89c69f733987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515,
          "referenced_widgets": [
            "9a48fa86718f4fd1a0c2a2fd2e8b60d2",
            "d5bea2d5fd9048e29a1abb2e6b235670",
            "a9abc821408f4c0ca11fa456c6de0736",
            "22c99f08e0bc4e2589d3f0d64f7845f0",
            "d52de9b1860f498cba57313b7beb4e65",
            "bc8c5cd7802d4ccb951d7f2350d453d1",
            "e8ead118eebe4ffe95ba201e846f889a",
            "592927edaeaf4461a474bd74fe89dbdd",
            "8f72124ad10b441193b65e45fc96ab62",
            "cdb79d653e6d4308b7b61e29eabf1720",
            "e51a42bdd8aa480fb69cff7e84f89cb3"
          ]
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a48fa86718f4fd1a0c2a2fd2e8b60d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "queVPwNqDNyq"
      },
      "outputs": [],
      "source": [
        "# del ddpm\n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Linear(40, 30)\n",
        "input = torch.randn(128, 40)\n",
        "output = (m(input))\n",
        "print(output.size())"
      ],
      "metadata": {
        "id": "eFYaDlei5Afd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "iqw1y3VOCbjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(128,40).long()\n",
        "input"
      ],
      "metadata": {
        "id": "LDiaPLh4Dald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(128*40, 128*40)\n",
        "embedding(input)"
      ],
      "metadata": {
        "id": "JDfwmRU9LvLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(2, 40, padding_idx=0)\n",
        "input = torch.LongTensor([[ 0,  0,  0,  0,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  0, 1,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  0, 0,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  1, 1,0,  0,  0,  0,0,0]])\n",
        "print(input)\n",
        "e = embedding(input)\n",
        "e.shape"
      ],
      "metadata": {
        "id": "H5cZqTxnMAY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr = torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).long()\n",
        "attr.unsqueeze(0)"
      ],
      "metadata": {
        "id": "DPRNN-x5NGZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr.repeat(32,1).shape"
      ],
      "metadata": {
        "id": "sHGjRde0swUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr = torch.tensor([[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).long()"
      ],
      "metadata": {
        "id": "T_YRK2L4t7Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr.shape"
      ],
      "metadata": {
        "id": "4jB0WQ6nRHcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.cat([attr,attr])\n",
        "c.shape"
      ],
      "metadata": {
        "id": "2Q3U23i2RIf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc = torch.cat([attr] * 2)\n",
        "cc.shape"
      ],
      "metadata": {
        "id": "VvTQOhQuRP3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xXHjSlT6RrDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "conditional_celeba.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a48fa86718f4fd1a0c2a2fd2e8b60d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5bea2d5fd9048e29a1abb2e6b235670",
              "IPY_MODEL_a9abc821408f4c0ca11fa456c6de0736",
              "IPY_MODEL_22c99f08e0bc4e2589d3f0d64f7845f0"
            ],
            "layout": "IPY_MODEL_d52de9b1860f498cba57313b7beb4e65"
          }
        },
        "d5bea2d5fd9048e29a1abb2e6b235670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc8c5cd7802d4ccb951d7f2350d453d1",
            "placeholder": "",
            "style": "IPY_MODEL_e8ead118eebe4ffe95ba201e846f889a",
            "value": "100%"
          }
        },
        "a9abc821408f4c0ca11fa456c6de0736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_592927edaeaf4461a474bd74fe89dbdd",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f72124ad10b441193b65e45fc96ab62",
            "value": 1000
          }
        },
        "22c99f08e0bc4e2589d3f0d64f7845f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdb79d653e6d4308b7b61e29eabf1720",
            "placeholder": "",
            "style": "IPY_MODEL_e51a42bdd8aa480fb69cff7e84f89cb3",
            "value": " 1000/1000 [02:02&lt;00:00,  8.19it/s]"
          }
        },
        "d52de9b1860f498cba57313b7beb4e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc8c5cd7802d4ccb951d7f2350d453d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ead118eebe4ffe95ba201e846f889a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "592927edaeaf4461a474bd74fe89dbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f72124ad10b441193b65e45fc96ab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdb79d653e6d4308b7b61e29eabf1720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51a42bdd8aa480fb69cff7e84f89cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}