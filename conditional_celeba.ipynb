{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/guided_diffusion/blob/main/conditional_celeba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "KC6FJYuwvRzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continue_training = True"
      ],
      "metadata": {
        "id": "HkwppPi9XqeI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, os.path\n",
        "\n",
        "first_run = True\n",
        "if os.path.exists('/content/data/celeba/img_align_celeba') and len(os.listdir('/content/data/celeba/img_align_celeba')) == 202599:\n",
        "  first_run = False"
      ],
      "metadata": {
        "id": "XUIaumn_zhLt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BNY6aXhoAnkb"
      },
      "outputs": [],
      "source": [
        "if first_run:\n",
        "  !rm -rf /content/images/*\n",
        "  !mkdir -p /content/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bIvCv_xS4nR3"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip uninstall matplotlib -y\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "KbaCeOgGtHsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9K3x1nQI2BS"
      },
      "source": [
        "### UNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "ldERGcvx4SjK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers[training]==0.1.3"
      ],
      "metadata": {
        "id": "75jejDLcKeC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel"
      ],
      "metadata": {
        "id": "_8ZSJZ_uLaZ4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "KhnJvTm4n_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CelebA"
      ],
      "metadata": {
        "id": "6oWvcCsVvAzz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2eEu3ZVZGJVa"
      },
      "outputs": [],
      "source": [
        "x_size = 32\n",
        "y_size = x_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rBJeO_gPGDsK"
      },
      "outputs": [],
      "source": [
        "tf = transforms.Compose(\n",
        "  [\n",
        "    transforms.Resize((x_size, y_size)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if first_run:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  !mkdir -p ./data\n",
        "  !rm -rf ./data/*\n",
        "  !cp -R ./drive/MyDrive/celeba/* ./data/celeba\n",
        "  !unzip \"./data/celeba/img_align_celeba.zip\" -d \"./data/celeba\""
      ],
      "metadata": {
        "id": "XHhMYmkYrqRi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1qLCertIF9wj"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "dataset = None\n",
        "result = None\n",
        "while result is None:\n",
        "    try:\n",
        "      # connect\n",
        "      dataset = CelebA(\n",
        "        \"./data/\",\n",
        "        download=False,\n",
        "        transform=tf,\n",
        "      )\n",
        "      result = True\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # break\n",
        "      time.sleep(60)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K3j-SLDjnDI0"
      },
      "outputs": [],
      "source": [
        "subset = torch.utils.data.Subset(dataset, range(0,320))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_ = 16\n",
        "eval_batch_size_ = 1"
      ],
      "metadata": {
        "id": "lz-S15b6AFU_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader = DataLoader(subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size_, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "qLZ0wYHb3hGS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qA4jjn7I43w"
      },
      "source": [
        "### Denoising Diffusion Probabilistic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Accelerate"
      ],
      "metadata": {
        "id": "uhwbA4szWTjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c29f638-05a6-41eb-e93b-f059db958a8e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Accelerate in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from Accelerate) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from Accelerate) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from Accelerate) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from Accelerate) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->Accelerate) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->Accelerate) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "\n",
        "\n",
        "class OldDDPMConditionalPipeline(DiffusionPipeline):\n",
        "    def __init__(self, unet: UNet2DConditionModel, scheduler):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(unet=unet, scheduler=scheduler)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, batch_size=1, generator=None, torch_device=None, output_type=\"pil\", hidden_states=None):\n",
        "        if torch_device is None:\n",
        "            torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        self.unet.to(torch_device)\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        image = torch.randn(\n",
        "            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n",
        "            generator=generator,\n",
        "        )\n",
        "        image = image.to(torch_device)\n",
        "\n",
        "        # set step values\n",
        "        self.scheduler.set_timesteps(1000)\n",
        "\n",
        "        for t in tqdm(self.scheduler.timesteps):\n",
        "            # 1. predict noise model_output\n",
        "            model_output = self.unet(image, t, hidden_states)[\"sample\"]\n",
        "\n",
        "            # 2. compute previous image: x_t -> t_t-1\n",
        "            image = self.scheduler.step(model_output, t, image)[\"prev_sample\"]\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}"
      ],
      "metadata": {
        "id": "6FAjtnErlOul"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26fo9mj0f9rP",
        "outputId": "f0593f3e-261a-4fa3-dba2-da67f4266c6b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "\n",
        "emb = nn.Embedding(2, 40, padding_idx=0)\n",
        "lin = nn.Linear(40, 1280)\n",
        "\n",
        "class DDPMConditionalPipeline(DiffusionPipeline):\n",
        "    def __init__(self, unet: UNet2DConditionModel, scheduler):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(unet=unet, scheduler=scheduler)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, guidance_scale=10.0, batch_size=1, generator=None, torch_device=None, output_type=\"pil\", hidden_states=None):\n",
        "        if torch_device is None:\n",
        "            torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        self.unet.to(torch_device)\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        latents = torch.randn(\n",
        "            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n",
        "            generator=generator,\n",
        "        )\n",
        "        image = latents.to(torch_device)\n",
        "\n",
        "        # set step values\n",
        "        self.scheduler.set_timesteps(1000)\n",
        "\n",
        "        for t in tqdm(self.scheduler.timesteps):\n",
        "            # 1. predict noise noise_pred\n",
        "            # latents_input = torch.cat([latents] * 2)\n",
        "            no_condition_attrs = torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).long()\n",
        "            y = lin(emb(no_condition_attrs)).to(torch_device)\n",
        "            # context = torch.cat([[y], [hidden_states]])\n",
        "            noise_prediction_text = self.unet(image, t, hidden_states)[\"sample\"]\n",
        "            noise_pred_uncond = self.unet(image, t, y)[\"sample\"]\n",
        "\n",
        "            # noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_prediction_text - noise_pred_uncond)\n",
        "\n",
        "\n",
        "            # 2. compute previous image: x_t -> t_t-1\n",
        "            image = self.scheduler.step(noise_pred, t, image)[\"prev_sample\"]\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}"
      ],
      "metadata": {
        "id": "prqVe251HpjY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "# from datasets import load_dataset\n",
        "# from diffusers import DDPMPipeline, DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    InterpolationMode,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "IdWgjyeuWRjJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p logs"
      ],
      "metadata": {
        "id": "GbxPdIHFW4Lj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"CelebA\")\n",
        "parser.add_argument(\"--dataset_name\", type=str, default=\"CelebA\")\n",
        "parser.add_argument(\"--dataset_config_name\", type=str, default=None)\n",
        "parser.add_argument(\"--train_data_dir\", type=str, default=None, help=\"A folder containing the training data.\")\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"diffusion_conditional\")\n",
        "parser.add_argument(\"--overwrite_output_dir\", action=\"store_true\")\n",
        "parser.add_argument(\"--cache_dir\", type=str, default=None)\n",
        "parser.add_argument(\"--resolution\", type=int, default=x_size)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=batch_size_)\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=eval_batch_size_)\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=100)\n",
        "parser.add_argument(\"--save_images_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--save_model_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--lr_scheduler\", type=str, default=\"cosine\")\n",
        "parser.add_argument(\"--lr_warmup_steps\", type=int, default=500)\n",
        "parser.add_argument(\"--adam_beta1\", type=float, default=0.95)\n",
        "parser.add_argument(\"--adam_beta2\", type=float, default=0.999)\n",
        "parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-6)\n",
        "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08)\n",
        "parser.add_argument(\"--use_ema\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--ema_inv_gamma\", type=float, default=1.0)\n",
        "parser.add_argument(\"--ema_power\", type=float, default=3 / 4)\n",
        "parser.add_argument(\"--ema_max_decay\", type=float, default=0.9999)\n",
        "parser.add_argument(\"--push_to_hub\", default=True)\n",
        "parser.add_argument(\"--use_auth_token\", action=\"store_true\")\n",
        "parser.add_argument(\"--hub_token\", type=str, default=None)\n",
        "parser.add_argument(\"--hub_model_id\", type=str, default=\"diffusion_conditional\")\n",
        "parser.add_argument(\"--hub_private_repo\", default=False)\n",
        "parser.add_argument(\"--logging_dir\", type=str, default=\"logs\")\n",
        "parser.add_argument(\n",
        "    \"--mixed_precision\",\n",
        "    type=str,\n",
        "    default=\"fp16\",\n",
        "    choices=[\"no\", \"fp16\", \"bf16\"],\n",
        "    help=(\n",
        "        \"Whether to use mixed precision. Choose\"\n",
        "        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
        "        \"and an Nvidia Ampere GPU.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "TMK26PrLXM7_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WuemTUnjbJMU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging_dir = os.path.join('.', '/logs')\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=args.mixed_precision,\n",
        "    log_with=\"tensorboard\",\n",
        "    logging_dir=logging_dir,\n",
        ")"
      ],
      "metadata": {
        "id": "YLuUoiMY5XB4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet2DConditionModel(\n",
        "    sample_size=args.resolution,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 128, 256, 256),\n",
        "    # block_out_channels=(128, 128, 256, 256, 512, 512),\n",
        "    down_block_types=( \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"DownBlock2D\"\n",
        "    ), \n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\"\n",
        "      ),\n",
        ")\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")"
      ],
      "metadata": {
        "id": "iijTrsJqVLXS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "if continue_training:\n",
        "  previous_model = \\\n",
        "    DDPMConditionalPipeline.from_pretrained('shalpin87/diffusion_conditional')\n",
        "  model = previous_model.unet\n",
        "  noise_scheduler = previous_model.scheduler"
      ],
      "metadata": {
        "id": "BHoPGLVtXnkE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=(args.adam_beta1, args.adam_beta2),\n",
        "    weight_decay=args.adam_weight_decay,\n",
        "    eps=args.adam_epsilon,\n",
        ")\n",
        "\n",
        "train_dataloader = dataloader\n",
        "lr_scheduler = get_scheduler(\n",
        "    args.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * args.num_epochs) // args.gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "ema_model = EMAModel(model, inv_gamma=args.ema_inv_gamma, power=args.ema_power, max_value=args.ema_max_decay)"
      ],
      "metadata": {
        "id": "2RUtbgXkgUQ4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.push_to_hub:\n",
        "    repo = init_git_repo(args, at_init=True)\n",
        "    repo.git_pull()"
      ],
      "metadata": {
        "id": "qC_zkxKK5eJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1469951-32bd-44db-a7fa-1c174d92e791"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/diffusion_conditional is already a clone of https://huggingface.co/shalpin87/diffusion_conditional. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "WARNING:huggingface_hub.repository:/content/diffusion_conditional is already a clone of https://huggingface.co/shalpin87/diffusion_conditional. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if accelerator.is_main_process:\n",
        "    run = os.path.split('.')[-1].split(\".\")[0]\n",
        "    accelerator.init_trackers(run)"
      ],
      "metadata": {
        "id": "accQ1syDH8g-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, y in (train_dataloader):\n",
        "  print(batch.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "aOtrmIqg2RRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0221f56b-4850-4f9f-8eda-5675c5257f59"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 32, 32])\n",
            "torch.Size([16, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0\n",
        "emb = nn.Embedding(2, 40, padding_idx=0)\n",
        "lin = nn.Linear(40, 1280)\n",
        "for epoch in range(args.num_epochs):\n",
        "    model.train()\n",
        "    progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "    # for step, batch in enumerate(train_dataloader):\n",
        "    for batch, batch_y in (train_dataloader):\n",
        "        # clean_images = batch[\"input\"]\n",
        "        clean_images = batch #[\"input\"]\n",
        "        # Sample noise that we'll add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "        bsz = clean_images.shape[0]\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_train_timesteps, (bsz,), device=clean_images.device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            # Predict the noise residual          \n",
        "            y = (lin(emb(batch_y.to(\"cpu\")))).to(\"cuda\")\n",
        "            noise_pred = model(noisy_images, timesteps, y)[\"sample\"]\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            if args.use_ema:\n",
        "                ema_model.step(model)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "        if args.use_ema:\n",
        "            logs[\"ema_decay\"] = ema_model.decay\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        accelerator.log(logs, step=global_step)\n",
        "        global_step += 1\n",
        "    progress_bar.close()\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # Generate sample images for visual inspection\n",
        "    # if accelerator.is_main_process:\n",
        "    if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        pipeline = DDPMConditionalPipeline(\n",
        "            unet=accelerator.unwrap_model(ema_model.averaged_model if args.use_ema else model),\n",
        "            scheduler=noise_scheduler,\n",
        "        )\n",
        "\n",
        "        generator = torch.manual_seed(0)\n",
        "        # run pipeline in inference (sample random noise and denoise)\n",
        "        attr = torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).long().repeat(eval_batch_size_,1)\n",
        "        y = (lin(emb(attr)))\n",
        "        images = pipeline(generator=generator, batch_size=args.eval_batch_size, output_type=\"numpy\", hidden_states=y.to(\"cuda\"))[\"sample\"]\n",
        "\n",
        "        # denormalize the images and save to tensorboard\n",
        "        images_processed = (images * 255).round().astype(\"uint8\")\n",
        "        imgs_t = images_processed.transpose(0, 3, 1, 2)\n",
        "        imgs_plt = images_processed.transpose(0, 1, 2, 3)\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        columns = 1\n",
        "        rows = 1\n",
        "        for i in range(1, columns*rows +1):\n",
        "            img = imgs_plt[i-1]\n",
        "            fig.add_subplot(rows, columns, i)\n",
        "            plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        # save the model\n",
        "        # if args.push_to_hub:\n",
        "        push_to_hub(args, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=False)\n",
        "        # else:\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "xDV9MhaIq20G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ce23ec1547064128b1533d0e54108b6e",
            "7bfaed006729447fa9e8177241ea60d1",
            "4bf51d139ce54acabf8488779eeb72a9",
            "6411e11467bf486cab272b11873e72ca",
            "53e1e96999904d51a38d7e56374f6685",
            "6b7fed56ef65402b8455e4b0d0dceb9b",
            "9cb3c5c6d06544658562db55dbcd1eb0",
            "bf845bc117b64df9aca4e36696d3a890",
            "daafb70e7bdd4f7cb1f4ef31bc341e7c",
            "ae284d22f19e45df8ab919f7050bc11d",
            "d9918f4e0fd548969093171b5ccb93ba"
          ]
        },
        "outputId": "aaaec97b-1dcd-4c9c-8988-9f0839340af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10174 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce23ec1547064128b1533d0e54108b6e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZLjKBSjJk1N"
      },
      "source": [
        "### Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "queVPwNqDNyq"
      },
      "outputs": [],
      "source": [
        "# del ddpm\n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Linear(40, 30)\n",
        "input = torch.randn(128, 40)\n",
        "output = (m(input))\n",
        "print(output.size())"
      ],
      "metadata": {
        "id": "eFYaDlei5Afd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "iqw1y3VOCbjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(128,40).long()\n",
        "input"
      ],
      "metadata": {
        "id": "LDiaPLh4Dald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(128*40, 128*40)\n",
        "embedding(input)"
      ],
      "metadata": {
        "id": "JDfwmRU9LvLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(2, 40, padding_idx=0)\n",
        "input = torch.LongTensor([[ 0,  0,  0,  0,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  0, 1,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  0, 0,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  1, 1,0,  0,  0,  0,0,0]])\n",
        "print(input)\n",
        "e = embedding(input)\n",
        "e.shape"
      ],
      "metadata": {
        "id": "H5cZqTxnMAY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr = torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).long()\n",
        "attr.unsqueeze(0)"
      ],
      "metadata": {
        "id": "DPRNN-x5NGZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr.repeat(32,1).shape"
      ],
      "metadata": {
        "id": "sHGjRde0swUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr = torch.tensor([[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).long()"
      ],
      "metadata": {
        "id": "T_YRK2L4t7Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr.shape"
      ],
      "metadata": {
        "id": "4jB0WQ6nRHcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.cat([attr,attr])\n",
        "c.shape"
      ],
      "metadata": {
        "id": "2Q3U23i2RIf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc = torch.cat([attr] * 2)\n",
        "cc.shape"
      ],
      "metadata": {
        "id": "VvTQOhQuRP3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xXHjSlT6RrDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "conditional_celeba.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce23ec1547064128b1533d0e54108b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bfaed006729447fa9e8177241ea60d1",
              "IPY_MODEL_4bf51d139ce54acabf8488779eeb72a9",
              "IPY_MODEL_6411e11467bf486cab272b11873e72ca"
            ],
            "layout": "IPY_MODEL_53e1e96999904d51a38d7e56374f6685"
          }
        },
        "7bfaed006729447fa9e8177241ea60d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7fed56ef65402b8455e4b0d0dceb9b",
            "placeholder": "​",
            "style": "IPY_MODEL_9cb3c5c6d06544658562db55dbcd1eb0",
            "value": "Epoch 0:   1%"
          }
        },
        "4bf51d139ce54acabf8488779eeb72a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf845bc117b64df9aca4e36696d3a890",
            "max": 10174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daafb70e7bdd4f7cb1f4ef31bc341e7c",
            "value": 106
          }
        },
        "6411e11467bf486cab272b11873e72ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae284d22f19e45df8ab919f7050bc11d",
            "placeholder": "​",
            "style": "IPY_MODEL_d9918f4e0fd548969093171b5ccb93ba",
            "value": " 106/10174 [01:02&lt;1:40:42,  1.67it/s, ema_decay=0.97, loss=0.0174, lr=2.12e-5, step=105]"
          }
        },
        "53e1e96999904d51a38d7e56374f6685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7fed56ef65402b8455e4b0d0dceb9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb3c5c6d06544658562db55dbcd1eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf845bc117b64df9aca4e36696d3a890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daafb70e7bdd4f7cb1f4ef31bc341e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae284d22f19e45df8ab919f7050bc11d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9918f4e0fd548969093171b5ccb93ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}