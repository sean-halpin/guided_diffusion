{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/guided_diffusion/blob/main/guided_diffusion_ddpm_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNY6aXhoAnkb"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/images/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llm5lF5Rx32V"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIvCv_xS4nR3"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip uninstall matplotlib -y\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "KbaCeOgGtHsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9K3x1nQI2BS"
      },
      "source": [
        "### UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UNET 1"
      ],
      "metadata": {
        "id": "doufxeNODSNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goLIIwvk5jOO"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# Simple Unet Structure.\n",
        "# \"\"\"\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class Conv3(nn.Module):\n",
        "#     def __init__(\n",
        "#         self, in_channels: int, out_channels: int, is_res: bool = False\n",
        "#     ) -> None:\n",
        "#         super().__init__()\n",
        "#         self.main = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "#             nn.GroupNorm(8, out_channels),\n",
        "#             nn.ReLU(),\n",
        "#         )\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "#             nn.GroupNorm(8, out_channels),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "#             nn.GroupNorm(8, out_channels),\n",
        "#             nn.ReLU(),\n",
        "#         )\n",
        "\n",
        "#         self.is_res = is_res\n",
        "\n",
        "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "#         x = self.main(x)\n",
        "#         if self.is_res:\n",
        "#             x = x + self.conv(x)\n",
        "#             return x / 1.414\n",
        "#         else:\n",
        "#             return self.conv(x)\n",
        "\n",
        "\n",
        "# class UnetDown(nn.Module):\n",
        "#     def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "#         super(UnetDown, self).__init__()\n",
        "#         layers =  [\n",
        "#                     Conv3(in_channels, out_channels), nn.MaxPool2d(2)\n",
        "#                   ]\n",
        "#         self.model = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "#         return self.model(x)\n",
        "\n",
        "\n",
        "# class UnetUp(nn.Module):\n",
        "#     def __init__(self, in_channels: int, out_channels: int) -> None:\n",
        "#         super(UnetUp, self).__init__()\n",
        "#         layers = [\n",
        "#             nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
        "#             Conv3(out_channels, out_channels),\n",
        "#             Conv3(out_channels, out_channels),\n",
        "#         ]\n",
        "#         self.model = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x: torch.Tensor, skip: torch.Tensor) -> torch.Tensor:\n",
        "#         x = torch.cat((x, skip), 1)\n",
        "#         x = self.model(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class TimeSiren(nn.Module):\n",
        "#     def __init__(self, emb_dim: int) -> None:\n",
        "#         super(TimeSiren, self).__init__()\n",
        "\n",
        "#         self.lin1 = nn.Linear(1, emb_dim, bias=False)\n",
        "#         self.lin2 = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "#         x = x.view(-1, 1)\n",
        "#         x = torch.sin(self.lin1(x))\n",
        "#         x = self.lin2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class NaiveUnet(nn.Module):\n",
        "#     def __init__(self, in_channels: int, out_channels: int, n_feat: int = 256, attribute_count=40) -> None:\n",
        "#         super(NaiveUnet, self).__init__()\n",
        "#         self.in_channels = in_channels\n",
        "#         self.out_channels = out_channels\n",
        "\n",
        "#         self.n_feat = n_feat\n",
        "\n",
        "#         self.init_conv = Conv3(in_channels, n_feat, is_res=True)\n",
        "\n",
        "#         self.labels = nn.Sequential(\n",
        "#             nn.Conv2d(attribute_count, n_feat, 3,2,32),\n",
        "#             nn.GroupNorm(8, n_feat),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "#         n_feat = n_feat*2\n",
        "#         self.down1 = UnetDown(n_feat, n_feat)\n",
        "#         self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
        "#         self.down3 = UnetDown(2 * n_feat, 2 * n_feat)\n",
        "\n",
        "#         self.to_vec = nn.Sequential(nn.AvgPool2d(4), nn.ReLU())\n",
        "\n",
        "#         self.timeembed = TimeSiren(2 * n_feat)\n",
        "\n",
        "#         self.up0 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 4, 4),\n",
        "#             nn.GroupNorm(8, 2 * n_feat),\n",
        "#             nn.ReLU(),\n",
        "#         )\n",
        "\n",
        "#         self.up1 = UnetUp(4 * n_feat, 2 * n_feat)\n",
        "#         self.up2 = UnetUp(4 * n_feat, n_feat)\n",
        "#         self.up3 = UnetUp(2 * n_feat, n_feat)\n",
        "#         self.out = nn.Conv2d(192, self.out_channels, 3, 1, 1)\n",
        "#         # self.out = nn.Conv2d(2 * n_feat, self.out_channels, 3, 1, 1)\n",
        "\n",
        "#     def forward(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "#         # print(\"X DEBUG\")\n",
        "#         # print(x.shape)\n",
        "#         xx = self.init_conv(x)\n",
        "#         # x = self.init_conv(x)\n",
        "#         # print(xx.shape)\n",
        "#         # print(\"Y DEBUG\")\n",
        "#         # print(y.shape)\n",
        "#         yy = y.view(y.shape[0], y.shape[1], 1,1)\n",
        "#         # print(yy.shape)\n",
        "#         yyy = self.labels(yy)\n",
        "#         # print(yyy.shape)\n",
        "#         # print(\"END DEBUG\")\n",
        "#         xy = torch.cat([xx,yyy], dim=1)\n",
        "\n",
        "#         down1 = self.down1(xy)\n",
        "#         # down1 = self.down1(x)\n",
        "#         down2 = self.down2(down1)\n",
        "#         down3 = self.down3(down2)\n",
        "\n",
        "#         thro = self.to_vec(down3)\n",
        "#         temb = self.timeembed(t).view(-1, self.n_feat * 4, 1, 1)\n",
        "#         # temb = self.timeembed(t).view(-1, self.n_feat * 2, 1, 1)\n",
        "\n",
        "#         thro = self.up0(thro + temb)\n",
        "\n",
        "#         up1 = self.up1(thro, down3) + temb\n",
        "#         up2 = self.up2(up1, down2)\n",
        "#         up3 = self.up3(up2, down1)\n",
        "#         # print(\"OUT DEBUG\")\n",
        "#         # print(up3.shape)\n",
        "#         # print(xx.shape)\n",
        "#         # print(x.shape)\n",
        "#         # print(\"END DEBUG\")\n",
        "#         out = self.out(torch.cat((up3, xx), 1))\n",
        "\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UNET 2"
      ],
      "metadata": {
        "id": "kAFe-JGdDXop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "# import math\n",
        "# from inspect import isfunction\n",
        "# from functools import partial\n",
        "\n",
        "# %matplotlib inline\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm.auto import tqdm\n",
        "# from einops import rearrange\n",
        "\n",
        "# import torch\n",
        "# from torch import nn, einsum\n",
        "# import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "s3A0G0Ohi8kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def exists(x):\n",
        "#     return x is not None\n",
        "\n",
        "# def default(val, d):\n",
        "#     if exists(val):\n",
        "#         return val\n",
        "#     return d() if isfunction(d) else d\n",
        "\n",
        "# class Residual(nn.Module):\n",
        "#     def __init__(self, fn):\n",
        "#         super().__init__()\n",
        "#         self.fn = fn\n",
        "\n",
        "#     def forward(self, x, *args, **kwargs):\n",
        "#         return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "# def Upsample(dim):\n",
        "#     return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "# def Downsample(dim):\n",
        "#     return nn.Conv2d(dim, dim, 4, 2, 1)"
      ],
      "metadata": {
        "id": "7HGnHB2Mi4rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SinusoidalPositionEmbeddings(nn.Module):\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.dim = dim\n",
        "\n",
        "#     def forward(self, time):\n",
        "#         device = time.device\n",
        "#         half_dim = self.dim // 2\n",
        "#         embeddings = math.log(10000) / (half_dim - 1)\n",
        "#         embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "#         embeddings = time[:, None] * embeddings[None, :]\n",
        "#         embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "#         return embeddings"
      ],
      "metadata": {
        "id": "Mj8gRN8si0Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Block(nn.Module):\n",
        "#     def __init__(self, dim, dim_out, groups = 8):\n",
        "#         super().__init__()\n",
        "#         self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "#         self.norm = nn.GroupNorm(groups, dim_out)\n",
        "#         self.act = nn.SiLU()\n",
        "\n",
        "#     def forward(self, x, scale_shift = None):\n",
        "#         x = self.proj(x)\n",
        "#         x = self.norm(x)\n",
        "\n",
        "#         if exists(scale_shift):\n",
        "#             scale, shift = scale_shift\n",
        "#             x = x * (scale + 1) + shift\n",
        "\n",
        "#         x = self.act(x)\n",
        "#         return x\n",
        "\n",
        "# class ResnetBlock(nn.Module):\n",
        "#     \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "    \n",
        "#     def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "#         super().__init__()\n",
        "#         self.mlp = (\n",
        "#             nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "#             if exists(time_emb_dim)\n",
        "#             else None\n",
        "#         )\n",
        "\n",
        "#         self.block1 = Block(dim, dim_out, groups=groups)\n",
        "#         self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "#         self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "#     def forward(self, x, time_emb=None):\n",
        "#         h = self.block1(x)\n",
        "\n",
        "#         if exists(self.mlp) and exists(time_emb):\n",
        "#             time_emb = self.mlp(time_emb)\n",
        "#             # h = rearrange(time_emb, \"b z c -> b c z 1\") + h\n",
        "#             h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "#         h = self.block2(h)\n",
        "#         return h + self.res_conv(x)\n",
        "    \n",
        "# class ConvNextBlock(nn.Module):\n",
        "#     \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
        "\n",
        "#     def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "#         super().__init__()\n",
        "#         self.mlp = (\n",
        "#             nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "#             if exists(time_emb_dim)\n",
        "#             else None\n",
        "#         )\n",
        "\n",
        "#         self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "#             nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "#             nn.GELU(),\n",
        "#             nn.GroupNorm(1, dim_out * mult),\n",
        "#             nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "#         )\n",
        "\n",
        "#         self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "#     def forward(self, x, time_emb=None):\n",
        "#         h = self.ds_conv(x)\n",
        "\n",
        "#         if exists(self.mlp) and exists(time_emb):\n",
        "#             assert exists(time_emb), \"time embedding must be passed in\"\n",
        "#             condition = self.mlp(time_emb)\n",
        "#             if condition.ndim == 2:\n",
        "#               h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "#             else:\n",
        "#               h = h + rearrange(condition, \"b z c -> b c z 1\")\n",
        "\n",
        "#         h = self.net(h)\n",
        "#         return h + self.res_conv(x)"
      ],
      "metadata": {
        "id": "elxBUp-Iixmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, dim, heads=4, dim_head=32):\n",
        "#         super().__init__()\n",
        "#         self.scale = dim_head**-0.5\n",
        "#         self.heads = heads\n",
        "#         hidden_dim = dim_head * heads\n",
        "#         self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "#         self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b, c, h, w = x.shape\n",
        "#         qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "#         q, k, v = map(\n",
        "#             lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "#         )\n",
        "#         q = q * self.scale\n",
        "\n",
        "#         sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "#         sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "#         attn = sim.softmax(dim=-1)\n",
        "\n",
        "#         out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "#         out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "#         return self.to_out(out)\n",
        "\n",
        "# class LinearAttention(nn.Module):\n",
        "#     def __init__(self, dim, heads=4, dim_head=32):\n",
        "#         super().__init__()\n",
        "#         self.scale = dim_head**-0.5\n",
        "#         self.heads = heads\n",
        "#         hidden_dim = dim_head * heads\n",
        "#         self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "#         self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "#                                     nn.GroupNorm(1, dim))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         b, c, h, w = x.shape\n",
        "#         qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "#         q, k, v = map(\n",
        "#             lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "#         )\n",
        "\n",
        "#         q = q.softmax(dim=-2)\n",
        "#         k = k.softmax(dim=-1)\n",
        "\n",
        "#         q = q * self.scale\n",
        "#         context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "#         out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "#         out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "#         return self.to_out(out)"
      ],
      "metadata": {
        "id": "q0xVHowHiue-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PreNorm(nn.Module):\n",
        "#     def __init__(self, dim, fn):\n",
        "#         super().__init__()\n",
        "#         self.fn = fn\n",
        "#         self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.norm(x)\n",
        "#         return self.fn(x)"
      ],
      "metadata": {
        "id": "GDPUYhP5irJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb?authuser=1#scrollTo=a5126e21\n",
        "# class Unet(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         dim,\n",
        "#         init_dim=None,\n",
        "#         out_dim=None,\n",
        "#         dim_mults=(1, 2, 4, 8),\n",
        "#         channels=3,\n",
        "#         with_time_emb=True,\n",
        "#         resnet_block_groups=8,\n",
        "#         use_convnext=True,\n",
        "#         convnext_mult=2,\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # determine dimensions\n",
        "#         self.channels = channels\n",
        "\n",
        "#         init_dim = default(init_dim, dim // 3 * 2)\n",
        "#         self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "#         dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "#         in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        \n",
        "#         if use_convnext:\n",
        "#             block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "#         else:\n",
        "#             block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "#         # time embeddings\n",
        "#         if with_time_emb:\n",
        "#             time_dim = dim * 4\n",
        "#             self.time_mlp = nn.Sequential(\n",
        "#                 SinusoidalPositionEmbeddings(dim),\n",
        "#                 nn.Linear(dim, time_dim),\n",
        "#                 nn.GELU(),\n",
        "#                 nn.Linear(time_dim, time_dim),\n",
        "#             )\n",
        "#         else:\n",
        "#             time_dim = None\n",
        "#             self.time_mlp = None\n",
        "\n",
        "#         # layers\n",
        "#         self.downs = nn.ModuleList([])\n",
        "#         self.ups = nn.ModuleList([])\n",
        "#         num_resolutions = len(in_out)\n",
        "\n",
        "#         for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "#             is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "#             self.downs.append(\n",
        "#                 nn.ModuleList(\n",
        "#                     [\n",
        "#                         block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "#                         block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "#                         Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "#                         Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "#                     ]\n",
        "#                 )\n",
        "#             )\n",
        "\n",
        "#         mid_dim = dims[-1]\n",
        "#         self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "#         self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "#         self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "#         for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "#             is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "#             self.ups.append(\n",
        "#                 nn.ModuleList(\n",
        "#                     [\n",
        "#                         block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "#                         block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "#                         Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "#                         Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "#                     ]\n",
        "#                 )\n",
        "#             )\n",
        "\n",
        "#         out_dim = default(out_dim, channels)\n",
        "#         self.final_conv = nn.Sequential(\n",
        "#             block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, time):\n",
        "#         x = self.init_conv(x)\n",
        "\n",
        "#         t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "#         h = []\n",
        "\n",
        "#         # downsample\n",
        "#         for block1, block2, attn, downsample in self.downs:\n",
        "#             x = block1(x, t)\n",
        "#             x = block2(x, t)\n",
        "#             x = attn(x)\n",
        "#             h.append(x)\n",
        "#             x = downsample(x)\n",
        "\n",
        "#         # bottleneck\n",
        "#         x = self.mid_block1(x, t)\n",
        "#         x = self.mid_attn(x)\n",
        "#         x = self.mid_block2(x, t)\n",
        "\n",
        "#         # upsample\n",
        "#         for block1, block2, attn, upsample in self.ups:\n",
        "#             x = torch.cat((x, h.pop()), dim=1)\n",
        "#             x = block1(x, t)\n",
        "#             x = block2(x, t)\n",
        "#             x = attn(x)\n",
        "#             x = upsample(x)\n",
        "\n",
        "#         return self.final_conv(x)"
      ],
      "metadata": {
        "id": "mxbBVsAsp0kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### UNET 4"
      ],
      "metadata": {
        "id": "0hybCg7EKcQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers "
      ],
      "metadata": {
        "id": "75jejDLcKeC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel"
      ],
      "metadata": {
        "id": "_8ZSJZ_uLaZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "KhnJvTm4n_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CelebA"
      ],
      "metadata": {
        "id": "6oWvcCsVvAzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eEu3ZVZGJVa"
      },
      "outputs": [],
      "source": [
        "x_size = 32\n",
        "y_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBJeO_gPGDsK"
      },
      "outputs": [],
      "source": [
        "tf = transforms.Compose(\n",
        "  [\n",
        "    transforms.Resize((x_size, y_size)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "XHhMYmkYrqRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./data\n",
        "!rm -rf ./data/*"
      ],
      "metadata": {
        "id": "w6asxj5GyN_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R ./drive/MyDrive/celeba/* ./data/celeba"
      ],
      "metadata": {
        "id": "xcC28Dacs8hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"./data/celeba/img_align_celeba.zip\" -d \"./data/celeba\""
      ],
      "metadata": {
        "id": "EW2_Vj-E1svi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLCertIF9wj"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "dataset = None\n",
        "result = None\n",
        "while result is None:\n",
        "    try:\n",
        "      # connect\n",
        "      dataset = CelebA(\n",
        "        \"./data/\",\n",
        "        download=False,\n",
        "        transform=tf,\n",
        "      )\n",
        "      result = True\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # break\n",
        "      time.sleep(60)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3j-SLDjnDI0"
      },
      "outputs": [],
      "source": [
        "subset = torch.utils.data.Subset(dataset, range(0,5120))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader = DataLoader(subset, batch_size=512, shuffle=True, num_workers=2)\n",
        "dataloader = DataLoader(dataset, batch_size=512, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "qLZ0wYHb3hGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qA4jjn7I43w"
      },
      "source": [
        "### Denoising Diffusion Probabilistic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Accelerate"
      ],
      "metadata": {
        "id": "uhwbA4szWTjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "# from datasets import load_dataset\n",
        "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    InterpolationMode,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "IdWgjyeuWRjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p logs"
      ],
      "metadata": {
        "id": "GbxPdIHFW4Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
        "parser.add_argument(\"--dataset_name\", type=str, default=None)\n",
        "parser.add_argument(\"--dataset_config_name\", type=str, default=None)\n",
        "parser.add_argument(\"--train_data_dir\", type=str, default=None, help=\"A folder containing the training data.\")\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"ddpm-model-64\")\n",
        "parser.add_argument(\"--overwrite_output_dir\", action=\"store_true\")\n",
        "parser.add_argument(\"--cache_dir\", type=str, default=None)\n",
        "parser.add_argument(\"--resolution\", type=int, default=x_size)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=16)\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=100)\n",
        "parser.add_argument(\"--save_images_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--save_model_epochs\", type=int, default=10)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--lr_scheduler\", type=str, default=\"cosine\")\n",
        "parser.add_argument(\"--lr_warmup_steps\", type=int, default=500)\n",
        "parser.add_argument(\"--adam_beta1\", type=float, default=0.95)\n",
        "parser.add_argument(\"--adam_beta2\", type=float, default=0.999)\n",
        "parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-6)\n",
        "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08)\n",
        "parser.add_argument(\"--use_ema\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--ema_inv_gamma\", type=float, default=1.0)\n",
        "parser.add_argument(\"--ema_power\", type=float, default=3 / 4)\n",
        "parser.add_argument(\"--ema_max_decay\", type=float, default=0.9999)\n",
        "parser.add_argument(\"--push_to_hub\", action=\"store_true\")\n",
        "parser.add_argument(\"--use_auth_token\", action=\"store_true\")\n",
        "parser.add_argument(\"--hub_token\", type=str, default=None)\n",
        "parser.add_argument(\"--hub_model_id\", type=str, default=None)\n",
        "parser.add_argument(\"--hub_private_repo\", action=\"store_true\")\n",
        "parser.add_argument(\"--logging_dir\", type=str, default=\"logs\")\n",
        "parser.add_argument(\n",
        "    \"--mixed_precision\",\n",
        "    type=str,\n",
        "    default=\"no\",\n",
        "    choices=[\"no\", \"fp16\", \"bf16\"],\n",
        "    help=(\n",
        "        \"Whether to use mixed precision. Choose\"\n",
        "        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
        "        \"and an Nvidia Ampere GPU.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "TMK26PrLXM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WuemTUnjbJMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from diffusers import DiffusionPipeline\n",
        "# ldm = DiffusionPipeline.from_pretrained('./ddpm-model-64')"
      ],
      "metadata": {
        "id": "2RdbijANqWN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "KC6FJYuwvRzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging_dir = os.path.join('.', '/logs')\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=args.mixed_precision,\n",
        "    log_with=\"tensorboard\",\n",
        "    logging_dir=logging_dir,\n",
        ")\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=args.resolution,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(32, 32, 64, 64), #, 128),\n",
        "    # block_out_channels=(128, 128, 256, 256, 512, 512),\n",
        "    down_block_types=(\n",
        "        # \"DownBlock2D\",\n",
        "        # \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        # \"UpBlock2D\",\n",
        "        # \"UpBlock2D\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=(args.adam_beta1, args.adam_beta2),\n",
        "    weight_decay=args.adam_weight_decay,\n",
        "    eps=args.adam_epsilon,\n",
        ")\n",
        "\n",
        "train_dataloader = dataloader\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    args.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * args.num_epochs) // args.gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "ema_model = EMAModel(model, inv_gamma=args.ema_inv_gamma, power=args.ema_power, max_value=args.ema_max_decay)\n",
        "\n",
        "if args.push_to_hub:\n",
        "    repo = init_git_repo(args, at_init=True)\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    run = os.path.split('.')[-1].split(\".\")[0]\n",
        "    accelerator.init_trackers(run)"
      ],
      "metadata": {
        "id": "iijTrsJqVLXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0\n",
        "for epoch in range(args.num_epochs):\n",
        "    model.train()\n",
        "    progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "    # for step, batch in enumerate(train_dataloader):\n",
        "    for batch, _ in (train_dataloader):\n",
        "        # clean_images = batch[\"input\"]\n",
        "        clean_images = batch #[\"input\"]\n",
        "        # Sample noise that we'll add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "        bsz = clean_images.shape[0]\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_train_timesteps, (bsz,), device=clean_images.device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            # Predict the noise residual\n",
        "            noise_pred = model(noisy_images, timesteps)[\"sample\"]\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            if args.use_ema:\n",
        "                ema_model.step(model)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "        if args.use_ema:\n",
        "            logs[\"ema_decay\"] = ema_model.decay\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        accelerator.log(logs, step=global_step)\n",
        "        global_step += 1\n",
        "    progress_bar.close()\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # Generate sample images for visual inspection\n",
        "    # if accelerator.is_main_process:\n",
        "    if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        pipeline = DDPMPipeline(\n",
        "            unet=accelerator.unwrap_model(ema_model.averaged_model if args.use_ema else model),\n",
        "            scheduler=noise_scheduler,\n",
        "        )\n",
        "\n",
        "        generator = torch.manual_seed(0)\n",
        "        # run pipeline in inference (sample random noise and denoise)\n",
        "        images = pipeline(generator=generator, batch_size=args.eval_batch_size, output_type=\"numpy\")[\"sample\"]\n",
        "\n",
        "        # denormalize the images and save to tensorboard\n",
        "        images_processed = (images * 255).round().astype(\"uint8\")\n",
        "        imgs_t = images_processed.transpose(0, 3, 1, 2)\n",
        "        imgs_plt = images_processed.transpose(0, 1, 2, 3)\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        columns = 4\n",
        "        rows = 4\n",
        "        for i in range(1, columns*rows +1):\n",
        "            img = imgs_plt[i-1]\n",
        "            fig.add_subplot(rows, columns, i)\n",
        "            plt.imshow(img)\n",
        "        plt.show()\n",
        "        accelerator.trackers[0].writer.add_images(\n",
        "            \"test_samples\", imgs_t, epoch\n",
        "        )\n",
        "\n",
        "    if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        # save the model\n",
        "        # if args.push_to_hub:\n",
        "            # push_to_hub(args, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=False)\n",
        "        # else:\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "        # pipeline.unet.push_to_hub(\"celeba_diffusion\", use_temp_dir=True)\n",
        "        # pipeline.scheduler.push_to_hub(\"celeba_diffusion\", use_temp_dir=True)\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "xDV9MhaIq20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZLjKBSjJk1N"
      },
      "source": [
        "### Train DDPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "queVPwNqDNyq"
      },
      "outputs": [],
      "source": [
        "# del ddpm\n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IWuWpk0VjRuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "guided_diffusion_ddpm_unet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}