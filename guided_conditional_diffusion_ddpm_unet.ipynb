{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-halpin/guided_diffusion/blob/main/guided_conditional_diffusion_ddpm_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNY6aXhoAnkb"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/images/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llm5lF5Rx32V"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIvCv_xS4nR3"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip uninstall matplotlib -y\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "KbaCeOgGtHsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9K3x1nQI2BS"
      },
      "source": [
        "### UNet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt -qq install git-lfs\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "ldERGcvx4SjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers[training]==0.1.3"
      ],
      "metadata": {
        "id": "75jejDLcKeC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel"
      ],
      "metadata": {
        "id": "_8ZSJZ_uLaZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "KhnJvTm4n_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CelebA"
      ],
      "metadata": {
        "id": "6oWvcCsVvAzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eEu3ZVZGJVa"
      },
      "outputs": [],
      "source": [
        "x_size = 32\n",
        "y_size = x_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBJeO_gPGDsK"
      },
      "outputs": [],
      "source": [
        "tf = transforms.Compose(\n",
        "  [\n",
        "    transforms.Resize((x_size, y_size)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "XHhMYmkYrqRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./data\n",
        "!rm -rf ./data/*"
      ],
      "metadata": {
        "id": "w6asxj5GyN_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R ./drive/MyDrive/celeba/* ./data/celeba"
      ],
      "metadata": {
        "id": "xcC28Dacs8hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"./data/celeba/img_align_celeba.zip\" -d \"./data/celeba\""
      ],
      "metadata": {
        "id": "EW2_Vj-E1svi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLCertIF9wj"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "\n",
        "dataset = None\n",
        "result = None\n",
        "while result is None:\n",
        "    try:\n",
        "      # connect\n",
        "      dataset = CelebA(\n",
        "        \"./data/\",\n",
        "        download=False,\n",
        "        transform=tf,\n",
        "      )\n",
        "      result = True\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      # break\n",
        "      time.sleep(60)\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3j-SLDjnDI0"
      },
      "outputs": [],
      "source": [
        "subset = torch.utils.data.Subset(dataset, range(0,32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_ = 32"
      ],
      "metadata": {
        "id": "lz-S15b6AFU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(subset, batch_size=32, shuffle=True, num_workers=2)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size_, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "qLZ0wYHb3hGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qA4jjn7I43w"
      },
      "source": [
        "### Denoising Diffusion Probabilistic Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Accelerate"
      ],
      "metadata": {
        "id": "uhwbA4szWTjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "\n",
        "\n",
        "class DDPMConditionalPipeline(DiffusionPipeline):\n",
        "    def __init__(self, unet: UNet2DConditionModel, scheduler):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(unet=unet, scheduler=scheduler)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, batch_size=1, generator=None, torch_device=None, output_type=\"pil\", hidden_states=None):\n",
        "        if torch_device is None:\n",
        "            torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        self.unet.to(torch_device)\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        image = torch.randn(\n",
        "            (batch_size, self.unet.in_channels, self.unet.sample_size, self.unet.sample_size),\n",
        "            generator=generator,\n",
        "        )\n",
        "        image = image.to(torch_device)\n",
        "\n",
        "        # set step values\n",
        "        self.scheduler.set_timesteps(1000)\n",
        "\n",
        "        for t in tqdm(self.scheduler.timesteps):\n",
        "            # 1. predict noise model_output\n",
        "            model_output = self.unet(image, t, hidden_states)[\"sample\"]\n",
        "\n",
        "            # 2. compute previous image: x_t -> t_t-1\n",
        "            image = self.scheduler.step(model_output, t, image)[\"prev_sample\"]\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image}"
      ],
      "metadata": {
        "id": "6FAjtnErlOul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "# from datasets import load_dataset\n",
        "# from diffusers import DDPMPipeline, DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel\n",
        "from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    InterpolationMode,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "IdWgjyeuWRjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p logs"
      ],
      "metadata": {
        "id": "GbxPdIHFW4Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
        "parser.add_argument(\"--dataset\", type=str, default=\"CelebA\")\n",
        "parser.add_argument(\"--dataset_name\", type=str, default=\"CelebA\")\n",
        "parser.add_argument(\"--dataset_config_name\", type=str, default=None)\n",
        "parser.add_argument(\"--train_data_dir\", type=str, default=None, help=\"A folder containing the training data.\")\n",
        "parser.add_argument(\"--output_dir\", type=str, default=\"diffusion_conditional\")\n",
        "parser.add_argument(\"--overwrite_output_dir\", action=\"store_true\")\n",
        "parser.add_argument(\"--cache_dir\", type=str, default=None)\n",
        "parser.add_argument(\"--resolution\", type=int, default=x_size)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=batch_size_)\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=batch_size_)\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=100)\n",
        "parser.add_argument(\"--save_images_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--save_model_epochs\", type=int, default=1)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--lr_scheduler\", type=str, default=\"cosine\")\n",
        "parser.add_argument(\"--lr_warmup_steps\", type=int, default=500)\n",
        "parser.add_argument(\"--adam_beta1\", type=float, default=0.95)\n",
        "parser.add_argument(\"--adam_beta2\", type=float, default=0.999)\n",
        "parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-6)\n",
        "parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08)\n",
        "parser.add_argument(\"--use_ema\", action=\"store_true\", default=True)\n",
        "parser.add_argument(\"--ema_inv_gamma\", type=float, default=1.0)\n",
        "parser.add_argument(\"--ema_power\", type=float, default=3 / 4)\n",
        "parser.add_argument(\"--ema_max_decay\", type=float, default=0.9999)\n",
        "parser.add_argument(\"--push_to_hub\", default=True)\n",
        "parser.add_argument(\"--use_auth_token\", action=\"store_true\")\n",
        "parser.add_argument(\"--hub_token\", type=str, default=None)\n",
        "parser.add_argument(\"--hub_model_id\", type=str, default=\"diffusion_conditional\")\n",
        "parser.add_argument(\"--hub_private_repo\", default=False)\n",
        "parser.add_argument(\"--logging_dir\", type=str, default=\"logs\")\n",
        "parser.add_argument(\n",
        "    \"--mixed_precision\",\n",
        "    type=str,\n",
        "    default=\"fp16\",\n",
        "    choices=[\"no\", \"fp16\", \"bf16\"],\n",
        "    help=(\n",
        "        \"Whether to use mixed precision. Choose\"\n",
        "        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
        "        \"and an Nvidia Ampere GPU.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "args = parser.parse_args()"
      ],
      "metadata": {
        "id": "TMK26PrLXM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WuemTUnjbJMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "KC6FJYuwvRzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from diffusers import DiffusionPipeline\n",
        "# ldm = DiffusionPipeline.from_pretrained('./ddpm-model-64')"
      ],
      "metadata": {
        "id": "2RdbijANqWN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging_dir = os.path.join('.', '/logs')\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=args.mixed_precision,\n",
        "    log_with=\"tensorboard\",\n",
        "    logging_dir=logging_dir,\n",
        ")"
      ],
      "metadata": {
        "id": "YLuUoiMY5XB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet2DConditionModel(\n",
        "    sample_size=args.resolution,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 128, 256, 256),\n",
        "    # block_out_channels=(128, 128, 256, 256, 512, 512),\n",
        "    down_block_types=( \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"CrossAttnDownBlock2D\", \n",
        "        \"DownBlock2D\"\n",
        "    ), \n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\", \n",
        "        \"CrossAttnUpBlock2D\"\n",
        "      ),\n",
        ")\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, tensor_format=\"pt\")\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=args.learning_rate,\n",
        "    betas=(args.adam_beta1, args.adam_beta2),\n",
        "    weight_decay=args.adam_weight_decay,\n",
        "    eps=args.adam_epsilon,\n",
        ")\n",
        "\n",
        "train_dataloader = dataloader\n",
        "lr_scheduler = get_scheduler(\n",
        "    args.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * args.num_epochs) // args.gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "ema_model = EMAModel(model, inv_gamma=args.ema_inv_gamma, power=args.ema_power, max_value=args.ema_max_decay)"
      ],
      "metadata": {
        "id": "iijTrsJqVLXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.push_to_hub:\n",
        "    repo = init_git_repo(args, at_init=True)\n",
        "    repo.git_pull()\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    run = os.path.split('.')[-1].split(\".\")[0]\n",
        "    accelerator.init_trackers(run)"
      ],
      "metadata": {
        "id": "qC_zkxKK5eJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, y in (train_dataloader):\n",
        "  print(batch.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "aOtrmIqg2RRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_step = 0\n",
        "emb = nn.Embedding(2, 40, padding_idx=0)\n",
        "lin = nn.Linear(40, 1280)\n",
        "for epoch in range(args.num_epochs):\n",
        "    model.train()\n",
        "    progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "    # for step, batch in enumerate(train_dataloader):\n",
        "    for batch, batch_y in (train_dataloader):\n",
        "        # clean_images = batch[\"input\"]\n",
        "        clean_images = batch #[\"input\"]\n",
        "        # Sample noise that we'll add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "        bsz = clean_images.shape[0]\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_train_timesteps, (bsz,), device=clean_images.device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            # Predict the noise residual          \n",
        "            y = (lin(emb(batch_y.to(\"cpu\")))).to(\"cuda\")\n",
        "            noise_pred = model(noisy_images, timesteps, y)[\"sample\"]\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            if args.use_ema:\n",
        "                ema_model.step(model)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "        if args.use_ema:\n",
        "            logs[\"ema_decay\"] = ema_model.decay\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        accelerator.log(logs, step=global_step)\n",
        "        global_step += 1\n",
        "    progress_bar.close()\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    # Generate sample images for visual inspection\n",
        "    # if accelerator.is_main_process:\n",
        "    if epoch % args.save_images_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        pipeline = DDPMConditionalPipeline(\n",
        "            unet=accelerator.unwrap_model(ema_model.averaged_model if args.use_ema else model),\n",
        "            scheduler=noise_scheduler,\n",
        "        )\n",
        "\n",
        "        generator = torch.manual_seed(0)\n",
        "        # run pipeline in inference (sample random noise and denoise)\n",
        "        attr = torch.tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]).long().repeat(32,1)\n",
        "        y = (lin(emb(attr)))\n",
        "        images = pipeline(generator=generator, batch_size=args.eval_batch_size, output_type=\"numpy\", hidden_states=y.to(\"cuda\"))[\"sample\"]\n",
        "\n",
        "        # denormalize the images and save to tensorboard\n",
        "        images_processed = (images * 255).round().astype(\"uint8\")\n",
        "        imgs_t = images_processed.transpose(0, 3, 1, 2)\n",
        "        imgs_plt = images_processed.transpose(0, 1, 2, 3)\n",
        "\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        columns = 4\n",
        "        rows = 4\n",
        "        for i in range(1, columns*rows +1):\n",
        "            img = imgs_plt[i-1]\n",
        "            fig.add_subplot(rows, columns, i)\n",
        "            plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    if epoch % args.save_model_epochs == 0 or epoch == args.num_epochs - 1:\n",
        "        # save the model\n",
        "        # if args.push_to_hub:\n",
        "        push_to_hub(args, pipeline, repo, commit_message=f\"Epoch {epoch}\", blocking=False)\n",
        "        # else:\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "xDV9MhaIq20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZLjKBSjJk1N"
      },
      "source": [
        "### Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "queVPwNqDNyq"
      },
      "outputs": [],
      "source": [
        "# del ddpm\n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Linear(40, 30)\n",
        "input = torch.randn(128, 40)\n",
        "output = (m(input))\n",
        "print(output.size())"
      ],
      "metadata": {
        "id": "eFYaDlei5Afd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "iqw1y3VOCbjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(128,40).long()\n",
        "input"
      ],
      "metadata": {
        "id": "LDiaPLh4Dald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(128*40, 128*40)\n",
        "embedding(input)"
      ],
      "metadata": {
        "id": "JDfwmRU9LvLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(2, 40, padding_idx=0)\n",
        "input = torch.LongTensor([[ 0,  0,  0,  0,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  0, 1,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  0, 0,0,  0,  0,  0,0,0],\n",
        "        [ 0,  0,  1, 1,0,  0,  0,  0,0,0]])\n",
        "print(input)\n",
        "e = embedding(input)\n",
        "e.shape"
      ],
      "metadata": {
        "id": "H5cZqTxnMAY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr = torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).long()\n",
        "attr.unsqueeze(0)"
      ],
      "metadata": {
        "id": "DPRNN-x5NGZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr.repeat(32,1).shape"
      ],
      "metadata": {
        "id": "sHGjRde0swUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T_YRK2L4t7Lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "guided_conditional_diffusion_ddpm_unet.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}